## 前提知識

基礎知識を知りたい人向けです。
ポケモンバトルの話は次からなので、読み飛ばしてもよい。

### 展開型同時手番二人ゼロサム有限確定不完全情報ゲーム

これはポケモンバトルのゲーム理論的な分類。

1. 展開型ゲーム: プレイヤーの選択によってゲーム展開が派生していくタイプのゲーム。ゲームの展開をゲーム木で表現できる。
    
2. 同時手番ゲーム: プレイヤーが同時に選択するタイプのゲーム。じゃんけん。
    
3. 二人ゲーム: 二人でやるタイプのゲーム。
    
4. ゼロサムゲーム: 自分の利益が他のプレイヤーの不利益になるタイプのゲーム。
    
5. 有限ゲーム: 有限の手数で必ず終わるタイプのゲーム。
    
6. 不確定ゲーム: 確率に左右されるタイプのゲーム。
    
7. 不完全情報ゲーム: ややこしいので詳しく説明する。

### 不完備情報と不完全情報について

いったん、ポケモンバトルの非公開情報について考える。  
クローズドチームシートでは技・特性・個体値・テラスタルタイプの全てが非公開で、オープンチームシートでも努力値配分は非公開である。  
判断に必要な情報が揃ってないので不完備情報ゲームである。

**不完備情報**: 判断をするうえで必要な情報が全て公開されてないこと

つぎに、それまで起こったことが完全に把握できるかだ。  
これに関してはログこそないが、把握できるかのように思える。  
しかしよく考えてほしい、ダメージの乱数を把握できない。  
そのため不完全情報ゲームであるといえる。

**不完全情報**: それまでに起こったことが全てのプレイヤーに公開されてないこと

### Game Tree（ゲーム木）

（完全情報ゲームは）互いがどの手を打ったかによってどのような局面が出現するかを場合分けしていくことでゲーム展開を樹形図にできる。ポケモンには確率的な要素があってややこしいが、理論上全局面を考えることはできる。このように現在の局面から出現するすべての局面の関係を樹形図にしたものをゲーム木と呼ぶ。

ゲーム木の全てを解析できれば最適な選択を考えることはできる。  
しかし、1ターン後ですら膨大な分岐が存在していて45秒で読み切ることは不可能である。
というかコンピュータでも無理なのでポケモンバトルで全探索は不可能だ。

### Minimax

Minimax法は、相手も自分も最善を尽くすと仮定して、自分の最適な行動を決定するための意思決定アルゴリズムである。特に、二人零和有限確定完全情報ゲームと呼ばれるもの（例：チェス、将棋、囲碁）で有効なアルゴリズムだが、基本的な考え方はポケモンバトルにも流用できる。

このアルゴリズムの基本的な方針は**起こり得る最大の損害を最小化する**ことである。  

Minimax法はゲーム木を全部探索するアルゴリズムだ、ポケモンバトルに対してナイーブに実装すると実行不可能なくらい時間がかかる。  

### Expectimax

Expectimax は 囲碁・将棋・チェスで使われるがポーカーでも有名で、完全情報ゲーム以外にも適用できるようになっている。

基本的にやってることは Minimax と同じで、違うのは勝つ確率を計算して期待値が高い方を最善手とするところだ。

### モンテカルロ木探索

モンテカルロ木探索 (Monte Carlo Tree Search: MCTS) は、特に複雑で巨大な探索空間を持つゲーム（囲碁、将棋、ポーカーなど）において、限られた計算資源の中で有望な手を見つけるためのヒューリスティックな探索アルゴリズムだ。不完全情報ゲームであるポケモンバトルにも応用が期待できる。

MCTSは主に次の**4つのステップ**を繰り返す：

1.  **選択 (Selection)**: ゲーム木を、現在の局面から、既にある程度探索されたノードの中から**有望そうなノード**を、**UCB (Upper Confidence Bound)** などの基準を用いて選択していきます。UCBは、探索回数が少ない（＝まだ価値が不確実）で、かつ、現状の勝率が高いノードを優先的に選ぶことで、「探索（Exploration）」と「活用（Exploitation）」のバランスを取る。
2.  **展開 (Expansion)**: 選択されたノード（局面）がまだ十分に展開されていない場合、そこから**未探索の子ノード**を1つ以上追加して展開する。
3.  **シミュレーション (Simulation/Rollout)**: 展開された子ノードから、ゲームが終了するまで**ランダムな**（または簡単なヒューリスティックに基づく）プレイアウト（対局シミュレーション）を行う。このランダムプレイアウトの結果が、そのノードの評価値となる。
4.  **バックプロパゲーション (Backpropagation)**: シミュレーションの結果（勝利/敗北）を、選択のステップで通ってきた親ノードすべてに遡って伝え、各ノードの**勝率**と**探索回数**を更新する。

MCTSの強みは、**評価関数を精密に設計する必要がない**点にある。大量のランダムシミュレーションを通じて、本当に勝率の高いルートを統計的に見つけ出すため、人間の思考に近い形で「あたり」をつけ、深掘りする。ポケモンにおいては、乱数や相手の非公開情報（技、持ち物）を確率的に考慮したシミュレーションを繰り返すことで、**不確定性を含む最善手**を導出する可能性があり、考察するのも面白いだろう。